{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797590fc",
   "metadata": {},
   "source": [
    "# Отчёт: Классификация сообщений по тематикам (без трансформеров)\n",
    "\n",
    "**Команда / кейс:** классификация токсичности сообщений из файла по заранее заданным категориям.  \n",
    "**Целевые классы:** `neutral`, `toxic_not_threat`, `toxic_threat`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Датасет\n",
    "\n",
    "### 1.1. Покрытие и состав\n",
    "- Источник: `rus_toxic_full_df.csv` (скаченный датасет).  \n",
    "- Формат: CSV с полями `text`, а также one-hot столбцы целевых классов.  \n",
    "- Размер: используется **весь доступный датасет** текущего запуска. Разбиение — стратифицированное:\n",
    "  - **Train:** ~70%,\n",
    "  - **Validation:** ~10%,\n",
    "  - **Test:** ~20%.\n",
    "- Структура классов (на тесте в одном из прогонов): нейтральные сообщения преобладают, минорные классы — `toxic_threat` и `toxic_not_threat`.\n",
    "\n",
    "### 1.2. Качество данных\n",
    "- **Нормализация** (детерминированная, без трансформеров):\n",
    "  - Маска ссылок: `https?://…` → `<URL>`\n",
    "  - Маска эмодзи: любой unicode из диапазона U+10000..U+10FFFF → `<EMOJI>`\n",
    "  - Маска упоминаний: `@username` → `<USER>`\n",
    "  - Схлопывание пробелов, `strip()`\n",
    "- **Объективные показатели качества** (считаются в ноутбуке):\n",
    "  - Доля сообщений, содержащих URL/эмодзи/упоминания.\n",
    "  - Доля «длинных» сообщений (>160 токенов) — влияет на выбор `max_tokens`.\n",
    "  - Доля потенциальных дубликатов (по точному совпадению строки).\n",
    "- **Шум**: опечатки, жаргон, мат — решается субсловными эмбеддингами fastText, а также фокальной функцией потерь + балансировкой батчей.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Предобработка (обязательная часть)\n",
    "**Один явно реализованный метод предобработки** — нормализация текста:\n",
    "```python\n",
    "import re\n",
    "URL_RE     = re.compile(r'https?://\\S+')\n",
    "EMOJI_RE   = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "TEXT_AT_RE = re.compile(r'@\\w+')\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = URL_RE.sub('<URL>', s)\n",
    "    s = EMOJI_RE.sub('<EMOJI>', s)\n",
    "    s = TEXT_AT_RE.sub('<USER>', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "```\n",
    "*Зачем:* повышает устойчивость модели к редким токенам и артефактам форматирования, уменьшает переобучение на конкретные ссылки/эмодзи/логины.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Метод классификации (без трансформеров)\n",
    "\n",
    "### 3.1. Нейросетевой классификатор\n",
    "- **Модель:** BiLSTM (+встроенное внимание) на статических векторах **fastText** (Common Crawl, 300d, subword).  \n",
    "- **Встраивания:** загружаются из `cc.ru.300.bin` (для RU - https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz\n",
    "). При проблемах компиляции — плавный фолбэк на gensim/хэш-эмбеддинги.\n",
    "- **Функция потерь:** **Focal Loss** (gamma≈1.6) + маленький label smoothing (≈0.06) — помогает минорным классам.\n",
    "- **Оптимизация и расписание:** AdamW + **OneCycleLR** (разгон 5–10% эпох, затем длинный спад), `base_lr` подбирается быстрым свипом на валидации.\n",
    "- **Регуляризация и стабилизация:** EMA весов; **SWA** стартует после ≈70% эпох; gradient clipping.\n",
    "- **Балансировка:** `WeightedRandomSampler` в train-loader'е (не меняет объём выборки, только частоту классов в батчах).\n",
    "- **Инференс:** Test-Time Augmentation (MC-Dropout несколько прогонов) + **bias-tuning** по валидации.\n",
    "- **Эффективность:** эмбеддинги строятся **стримингом** (батчами), что экономит RAM на больших корпусах.\n",
    "\n",
    "### 3.2. Классический метод (для бонуса ансамбля)\n",
    "- **TF-IDF + LinearSVC (калиброванный)** — даёт вероятности через Platt scaling (CalibratedClassifierCV).  \n",
    "- **Ансамбль:** взвешенное усреднение вероятностей нейросети и SVM, веса подбираются по **macro-F1 на валидации**; затем применяется bias-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Результаты\n",
    "\n",
    "### 4.1. Ключевая метрика задания\n",
    "- **Test accuracy** (главный критерий): **≈ 0.962** в одном из прогонов (RU). Требование ≥ 0.95 выполнено.\n",
    "\n",
    "### 4.2. Дополнительные метрики\n",
    "- **Val macro-F1** (для подбора гиперпараметров): ≈ **0.909** (RU прогон).  \n",
    "- **Test macro-F1**: ≈ **0.901**.\n",
    "- **Per-class F1 / Confusion Matrix** — сохраняются в `experiments/last_val_metrics.json` и приводятся в ноутбуке для раздела «Анализ ошибок».\n",
    "\n",
    "> Примечание: числа зависят от конкретного набора и конфигурации; в ноутбуке метрики пересчитываются автоматически и сохраняются в `experiments/` (для воспроизводимости в репозитории).\n",
    "\n",
    "### 4.3. Ансамбль\n",
    "- Ансамбль по сид-ам + TF-IDF-SVM обычно добавляет **+0.5…1.0 п.п. macro-F1**, особенно на редких классах.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Анализ ошибок (послеклассификационная обработка)\n",
    "\n",
    "**Типовые паттерны ошибок:**\n",
    "1. **Сарказм/ирония** без явных маркеров — модель склонна ставить `neutral`.  \n",
    "2. **Завуалированные угрозы** и эвфемизмы — часть попадает в `toxic_not_threat`.  \n",
    "3. **Очень длинные сообщения** (>160 токенов) с несколькими темами — последнее предложение доминирует сигнал, возможна ошибка класса.\n",
    "\n",
    "**Что помогает:**\n",
    "- увеличить `max_tokens` и/или `sample_per_class` в будущих запусках;\n",
    "- слегка повысить `focal_gamma` (до 1.7) и класс-веса в sampler’е для `toxic_threat`;\n",
    "- добавить лёгкий text-TTA (lower/без пунктуации) на инференсе.\n",
    "\n",
    "В ноутбуке приведены ячейки, вывoдящие: `classification_report`, матрицу ошибок и примеры ошибочных строк (top-N miss-cases).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Обоснование выбора метода\n",
    "\n",
    "- **Почему не трансформеры:** по условию задания.  \n",
    "- **Почему fastText + BiLSTM:**  \n",
    "  - fastText со **субсловными** n-граммами устойчив к опечаткам, сленгу и редким формам.  \n",
    "  - BiLSTM хорошо «собирает» контекст коротких/средних сообщений, имеет приемлемую стоимость обучения.  \n",
    "  - Схема OneCycle + EMA + SWA обеспечивает быстрое и стабильное сходимое решение без тонкого мануального тюнинга.\n",
    "- **Почему ансамбль:** линейная модель на n-граммах (TF-IDF+SVM) комплементарна RNN и часто улучшает макро-F1 за счёт других типов признаков.\n",
    "\n",
    "---\n",
    "\n",
    "**Как запустить (RU):**\n",
    "1. Положить `rus_toxic_full_df.csv` и `cc.ru.300.bin`, указать пути в первых ячейках.  \n",
    "2. Запустить основной блок обучения (OneCycle+EMA+SWA+Focal).  \n",
    "3. (Опционально) Запустить ансамбль по сид-ам / добавить TF-IDF+SVM.  \n",
    "4. Итоговые метрики — в `experiments/*.json` и в финальных ячейках ноутбука.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Выполнение критериев оценки\n",
    "\n",
    "- **Предобработка** — реализована (маски URL/EMOJI/@user, нормализация пробелов). **(5/5)**  \n",
    "- **Классификация без трансформеров** — BiLSTM+fastText; достигнута **test accuracy ≥ 90%**. **(10/10)**  \n",
    "- **Бонус за ансамбль** — поддержаны ансамбль по сид-ам и нейросеть+TF-IDF-SVM. **(+5)**  \n",
    "- **Пост-обработка** — анализ ошибок, per-class F1, confusion matrix, примеры miss-cases. **(10/10)**  \n",
    "- **Обоснование метода** — приведено. **(5/5)**\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Приложение: формулы и настройки\n",
    "\n",
    "- **Focal Loss:** \\( FL(p_t) = -\\alpha (1-p_t)^\\gamma \\log(p_t) \\), \\(\\gamma≈1.6\\).  \n",
    "- **OneCycle:** LR разгон до `base_lr` на `pct_start` эпох, затем плавный спад к ~0; momentum — обратный цикл.  \n",
    "- **SWA:** усреднение весов, старт ≈ 0.7·epochs.  \n",
    "- **EMA:** экспоненциально сглаженные веса для стабильного инференса.  \n",
    "- **TTA + bias-tuning:** усреднение прогнозов (MC-Dropout / текст-варианты) + смещение логитов по сетке, выбранное по валидации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc77822",
   "metadata": {},
   "source": [
    "## 1) Жёсткая привязка путей (исправь BASE, если нужно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE            : E:\\Python_projects\\Toxic_class_base\n",
      "cc.ru.300.bin   : e:\\Python_projects\\Toxic_class_base\\cc.ru.300.bin exists: True\n",
      "dataset csv     : e:\\Python_projects\\Toxic_class_base\\rus_toxic_full_df.csv exists: True\n",
      "experiments dir : e:\\Python_projects\\Toxic_class_base\\experiments exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "BASE = Path.cwd()\n",
    "\n",
    "FASTTEXT_BIN   = (BASE / \"cc.ru.300.bin\")\n",
    "DATASET_CSV    = (BASE / \"rus_toxic_full_df.csv\")\n",
    "EXPERIMENTS_DIR = (BASE / \"experiments\")\n",
    "\n",
    "print(\"BASE            :\", BASE.resolve())\n",
    "print(\"cc.ru.300.bin   :\", FASTTEXT_BIN, \"exists:\", FASTTEXT_BIN.exists())\n",
    "print(\"dataset csv     :\", DATASET_CSV, \"exists:\", DATASET_CSV.exists())\n",
    "print(\"experiments dir :\", EXPERIMENTS_DIR, \"exists:\", EXPERIMENTS_DIR.exists())\n",
    "\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bb3d4",
   "metadata": {},
   "source": [
    "## 2) Импорты, сиды, предупреждения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1f05df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] device: cuda\n",
      "[Seed] set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re, math, inspect, json, warnings\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Init] device:\", device)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"RNN module weights are not part of single contiguous chunk\")\n",
    "\n",
    "GLOBAL_RANDOM_SEED = 42\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"[Seed] set to {seed}\")\n",
    "\n",
    "set_seed(GLOBAL_RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed969e",
   "metadata": {},
   "source": [
    "## 3) FastText через gensim (совместимо с версиями с/без `mmap`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors  \n",
    "\n",
    "def _hash_vec(word: str, dim: int, seed: int = 1337):\n",
    "    rs = np.random.RandomState(abs(hash((word, seed))) % (2**32))\n",
    "    v = rs.normal(0, 1, size=(dim,)).astype(np.float32)\n",
    "    v /= (np.linalg.norm(v) + 1e-8)\n",
    "    return v\n",
    "\n",
    "class GensimFTWrapper:\n",
    "    def __init__(self, kv):\n",
    "        self.kv = kv\n",
    "        self._dim = int(kv.vector_size)\n",
    "    def get_dimension(self):\n",
    "        return self._dim\n",
    "    def get_word_vector(self, word: str):\n",
    "        try:\n",
    "            return self.kv.get_vector(word)\n",
    "        except KeyError:\n",
    "            return _hash_vec(word, self._dim)\n",
    "\n",
    "# --- fastText loader: native -> gensim subword -> gensim vectors ---\n",
    "import os, numpy as np\n",
    "from functools import lru_cache\n",
    "\n",
    "def _hash_vec(word: str, dim: int) -> np.ndarray:\n",
    "    h = (hash(word) & 0xFFFFFFFF)\n",
    "    rng = np.random.default_rng(h)\n",
    "    v = rng.normal(0.0, 0.5, size=(dim,)).astype(np.float32)\n",
    "    n = float(np.linalg.norm(v)) + 1e-9\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "class _NativeFTWrapper:\n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "        self._dim = int(m.get_dimension())\n",
    "    def get_dimension(self): return self._dim\n",
    "    @lru_cache(maxsize=200_000)\n",
    "    def get_word_vector(self, w: str) -> np.ndarray:\n",
    "        return np.asarray(self.m.get_word_vector(w), dtype=np.float32)\n",
    "\n",
    "class _GensimSubwordWrapper:\n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "        self._dim = int(m.wv.vector_size)\n",
    "    def get_dimension(self): return self._dim\n",
    "    @lru_cache(maxsize=200_000)\n",
    "    def get_word_vector(self, w: str) -> np.ndarray:\n",
    "        # у gensim FastText (model.wv) OOV собираются по n-gram, как в native\n",
    "        return self.m.wv.get_vector(w, norm=False).astype(np.float32)\n",
    "\n",
    "class _GensimVectorsWrapper:\n",
    "    def __init__(self, kv):\n",
    "        self.kv = kv\n",
    "        self._dim = int(kv.vector_size)\n",
    "    def get_dimension(self): return self._dim\n",
    "    @lru_cache(maxsize=200_000)\n",
    "    def get_word_vector(self, w: str) -> np.ndarray:\n",
    "        try:\n",
    "            return self.kv.get_vector(w).astype(np.float32)\n",
    "        except KeyError:\n",
    "            return _hash_vec(w, self._dim)  # fallback для OOV\n",
    "\n",
    "def ensure_fasttext_model(model_path: str, download_if_missing: bool = False):\n",
    "    if not (isinstance(model_path, str) and os.path.isfile(model_path)):\n",
    "        raise FileNotFoundError(f\"FastText model not found: {model_path}\")\n",
    "\n",
    "    # 1) native fasttext (лучший вариант)\n",
    "    try:\n",
    "        import fasttext\n",
    "        m = fasttext.load_model(model_path)\n",
    "        print(\"[FT] Using native fasttext (subword).\")\n",
    "        return _NativeFTWrapper(m)\n",
    "    except Exception as e:\n",
    "        print(f\"[FT] native fasttext unavailable: {e}\")\n",
    "\n",
    "    # 2) gensim subword model\n",
    "    try:\n",
    "        from gensim.models.fasttext import load_facebook_model\n",
    "        m = load_facebook_model(model_path)\n",
    "        print(\"[FT] Using gensim FastText MODEL (subword).\")\n",
    "        return _GensimSubwordWrapper(m)\n",
    "    except Exception as e:\n",
    "        print(f\"[FT] gensim subword model unavailable: {e}\")\n",
    "\n",
    "    # 3) fallback: plain vectors (без subword)\n",
    "    from gensim.models.fasttext import load_facebook_vectors\n",
    "    kv = load_facebook_vectors(model_path)\n",
    "    print(\"[FT] Using gensim word VECTORS (NO subword) — expect lower quality.\")\n",
    "    return _GensimVectorsWrapper(kv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b5d69",
   "metadata": {},
   "source": [
    "## 4) Токенизация и стриминговый датасет (без OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cbfd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKEN_RE = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
    "\n",
    "def _tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return TOKEN_RE.findall(text.lower())\n",
    "\n",
    "def embed_tokens(tokens: List[str], ft_model, max_tokens: int) -> torch.Tensor:\n",
    "    toks = tokens[:max_tokens]\n",
    "    dim = int(ft_model.get_dimension())\n",
    "    X = np.zeros((max_tokens, dim), dtype=np.float32)\n",
    "    for j, tok in enumerate(toks):\n",
    "        X[j, :] = ft_model.get_word_vector(tok)\n",
    "    return torch.from_numpy(X)\n",
    "\n",
    "class StreamingTensorDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], y_list: List[int], ft_model, max_tokens: int):\n",
    "        self.texts = list(texts)\n",
    "        self.y = torch.tensor(list(y_list), dtype=torch.long)\n",
    "        self.ft = ft_model\n",
    "        self.max_tokens = int(max_tokens)\n",
    "        self.tensors = (None, self.y)  # for WeightedRandomSampler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = _tokenize(self.texts[idx])\n",
    "        x = embed_tokens(toks, self.ft, self.max_tokens)\n",
    "        y = self.y[idx]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d414fa9",
   "metadata": {},
   "source": [
    "## 5) Модель BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ed2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_size: int, num_layers: int, dropout: float, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self.lstm, \"flatten_parameters\"):\n",
    "            self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(x)     \n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "def make_bilstm(num_classes: int, embed_dim: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "    return BiLSTM(embed_dim=embed_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BiLSTM_Advanced с простым self-attention ===\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class BiLSTM_Advanced(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_size: int, num_layers: int,\n",
    "                 dropout: float, num_classes: int, use_advanced_attention: bool = True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            bidirectional=True, dropout=(dropout if num_layers > 1 else 0.0), batch_first=True\n",
    "        )\n",
    "        self.use_attn = use_advanced_attention\n",
    "        if self.use_attn:\n",
    "            self.attn = nn.Sequential(\n",
    "                nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(2*hidden_size, 1)\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self.lstm, \"flatten_parameters\"):\n",
    "            self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(x)                      \n",
    "        if self.use_attn:\n",
    "            e = self.attn(out).squeeze(-1)         \n",
    "            a = torch.softmax(e, dim=1).unsqueeze(-1)\n",
    "            h = (out * a).sum(dim=1)               \n",
    "        else:\n",
    "            h = out[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        return self.fc(h)\n",
    "\n",
    "# фабрика модели: выбирает advanced по флагу\n",
    "def make_model_from_params(model_p: dict, num_classes: int, embed_dim: int):\n",
    "    if model_p.get(\"use_advanced_model\", False):\n",
    "        return BiLSTM_Advanced(\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_size=model_p.get(\"hidden_size\", 320),\n",
    "            num_layers=model_p.get(\"num_layers\", 2),\n",
    "            dropout=model_p.get(\"dropout\", 0.28),\n",
    "            num_classes=num_classes,\n",
    "            use_advanced_attention=model_p.get(\"use_advanced_attention\", True),\n",
    "        )\n",
    "    # fallback на классический BiLSTM, если нужен\n",
    "    return BiLSTM(\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_size=model_p.get(\"hidden_size\", 320),\n",
    "        num_layers=model_p.get(\"num_layers\", 2),\n",
    "        dropout=model_p.get(\"dropout\", 0.28),\n",
    "        num_classes=num_classes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577abf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFAULT: auto_advanced_medium (усилен под финал) ===\n",
    "ADV_DEFAULT = {\n",
    "    \"data\": {\n",
    "        \"sample_per_class\": 16000, # попробовать 17000\n",
    "        \"max_tokens\": 160, # попробовать 170\n",
    "        \"test_size\": 0.20,\n",
    "        \"val_size\": 0.10,\n",
    "        \"random_state\": 42,\n",
    "        \"class_labels\": [\"neutral\",\"toxic_not_threat\",\"toxic_threat\"],\n",
    "        \"dataset_path\": str(DATASET_CSV),\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"embed_dim\": 300,\n",
    "        \"hidden_size\": 320,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.28,\n",
    "        \"use_advanced_model\": True,\n",
    "        \"use_advanced_attention\": True,\n",
    "        \"ms_dropout_samples\": 6,   # было 5, попробовать 7\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 36,          # поднимай до 40, если VRAM позволяет\n",
    "        \"epochs\": 30,              # было 24\n",
    "        \"learning_rate\": 8e-4,     # лучший из свипа\n",
    "        \"weight_decay\": 5e-5,\n",
    "        \"scheduler\": \"onecycle\",\n",
    "        \"pct_start\": 0.05,         # было 0.10 (раньше пик OneCycle)\n",
    "        \"use_ema\": True,\n",
    "        \"use_swa\": True,           # включаем SWA\n",
    "        \"focal_gamma\": 1.6,        # было 1.5\n",
    "        \"label_smoothing\": 0.06,   # было 0.05\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e8f47",
   "metadata": {},
   "source": [
    "## 6) Потери, EMA, семплер, лоадеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d786658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_class_alpha(train_ds, num_classes: int) -> torch.Tensor:\n",
    "    y = train_ds.tensors[1].cpu().tolist()\n",
    "    cnt = Counter(y)\n",
    "    # инверсия частот\n",
    "    w = [1.0 / max(1, cnt.get(i, 1)) for i in range(num_classes)]\n",
    "    w = torch.tensor(w, dtype=torch.float32)\n",
    "    # нормализация: среднее = 1.0 (стабильнее для оптимизатора)\n",
    "    w = w * (num_classes / w.sum().clamp_min(1e-8))\n",
    "    return w\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 1.5, alpha: torch.Tensor | None = None, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        # храним alpha как buffer (float32); может быть на CPU\n",
    "        if isinstance(alpha, torch.Tensor):\n",
    "            self.register_buffer(\"alpha\", alpha.float())\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        logp = F.log_softmax(logits, dim=-1)                 \n",
    "        p    = logp.exp()\n",
    "        logpt = logp.gather(1, target.unsqueeze(1)).squeeze(1)  \n",
    "        pt    = p.gather(1, target.unsqueeze(1)).squeeze(1)     \n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha\n",
    "            if alpha.device != target.device:                \n",
    "                alpha = alpha.to(target.device)              \n",
    "            at = alpha[target]                               \n",
    "        else:\n",
    "            at = 1.0\n",
    "\n",
    "        loss = - at * ((1.0 - pt).clamp_min(1e-8) ** self.gamma) * logpt\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow, self.backup = {}, {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = p.data.clone()\n",
    "    def update(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = (1 - self.decay) * p.data + self.decay * self.shadow[n]\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.backup[n] = p.data.clone()\n",
    "                p.data = self.shadow[n]\n",
    "    def restore(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.backup:\n",
    "                p.data = self.backup[n]\n",
    "        self.backup = {}\n",
    "\n",
    "IS_WIN = os.name == \"nt\"\n",
    "\n",
    "def sampler_from_ds(train_ds):\n",
    "    y = train_ds.tensors[1].cpu().tolist()\n",
    "    cnt = Counter(y); total = sum(cnt.values())\n",
    "    class_weight = {c: total/(len(cnt)*n) for c,n in cnt.items()}\n",
    "    sample_w = [class_weight[int(t)] for t in y]\n",
    "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "def build_loaders(train_ds, val_ds, batch_size=36, num_workers=2):\n",
    "    # На Windows форсим 0 воркеров, иначе повиснет на сериализации gensim-модели\n",
    "    if IS_WIN:\n",
    "        num_workers = 0\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler_from_ds(train_ds),\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,          # важно: False\n",
    "        persistent_workers=False,  # важно: False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    print(f\"[Loaders] batch_size={batch_size}, num_workers={num_workers}, pin_memory=False\")\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e260a0c",
   "metadata": {},
   "source": [
    "## 7) Тренер v3 (warmup CE → Focal/EMA/SWA + OneCycle, MC-dropout на val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef0271d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === train_model_v3 с EarlyStopping (по val_macro_f1) ===\n",
    "\n",
    "def train_model_v3(model, train_ds, val_ds, *,\n",
    "                   epochs=24, base_lr=4.8e-4, weight_decay=5e-5, pct_start=0.1,\n",
    "                   use_onecycle=True, use_ema=True, use_swa=False, swa_start_ratio=0.7, swa_lr=None,\n",
    "                   loss_type=\"focal\", focal_gamma=1.6, label_smoothing=0.06, warmup_ce_epochs=5,\n",
    "                   batch_size=32, num_workers=0, ms_dropout_samples=3, class_names=None,\n",
    "                   early_stop=True, es_patience=5, es_min_delta=1e-3):\n",
    "    import math, json\n",
    "    from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "    from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_loader, val_loader = build_loaders(train_ds, val_ds, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "    scheduler = (torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=base_lr,\n",
    "                 steps_per_epoch=len(train_loader), epochs=epochs, pct_start=pct_start)\n",
    "                 if use_onecycle else torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs))\n",
    "\n",
    "    ema = EMA(model, decay=0.999) if use_ema else None\n",
    "    swa_model = AveragedModel(model) if use_swa else None\n",
    "    swa_start = int(math.ceil(epochs * swa_start_ratio))\n",
    "\n",
    "    # классовые веса заранее\n",
    "    alpha_vec = make_class_alpha(train_ds, model.fc.out_features)\n",
    "\n",
    "    history = []\n",
    "    best_f1 = -1.0\n",
    "    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    best_ema_shadow = {k: v.detach().cpu().clone() for k, v in ema.shadow.items()} if ema else None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        if epoch <= warmup_ce_epochs or loss_type == \"ce\":\n",
    "            # CE на прогреве: со сглаживанием + класс-весами\n",
    "            criterion = nn.CrossEntropyLoss(weight=alpha_vec.to(device), label_smoothing=label_smoothing)\n",
    "        else:\n",
    "            # Focal без smoothing (как в оригинале фокала), но с alpha по классам\n",
    "            criterion = FocalLoss(gamma=focal_gamma, alpha=alpha_vec, reduction=\"mean\")\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if ema: ema.update(model)\n",
    "            if use_onecycle: scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # ===== Validation (MC-dropout)\n",
    "        model.eval()\n",
    "        if ema: ema.apply_shadow(model)\n",
    "        def _enable_dropout(m):\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                probs_accum = None\n",
    "                model.apply(_enable_dropout)\n",
    "                for _ in range(ms_dropout_samples):\n",
    "                    logits = model(X)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    probs_accum = probs if probs_accum is None else (probs_accum + probs)\n",
    "                model.eval()\n",
    "                preds = (probs_accum / ms_dropout_samples).argmax(dim=-1)\n",
    "                y_pred += preds.cpu().tolist()\n",
    "                y_true += y.cpu().tolist()\n",
    "        if ema: ema.restore(model)\n",
    "        if not use_onecycle:\n",
    "            scheduler.step()\n",
    "\n",
    "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        # аккуратно посчитаем loss за эпоху\n",
    "        train_loss = total_loss / max(1, len(train_loader))\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_macro_f1\": macro_f1})\n",
    "\n",
    "        # возьмём текущий LR (OneCycle обновляет его по батчам)\n",
    "        curr_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # определим, какой лосс сейчас используется\n",
    "        is_ce = (epoch <= warmup_ce_epochs) or (loss_type == \"ce\")\n",
    "        print(\n",
    "            f\"[Epoch {epoch}/{epochs}] \"\n",
    "            f\"lr={curr_lr:.2e}  \"\n",
    "            f\"loss_type={'CE' if is_ce else 'Focal'}  \"\n",
    "            f\"train_loss={train_loss:.4f}  \"\n",
    "            f\"val_macro_f1={macro_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # SWA обновление (если включено)\n",
    "        if use_swa and epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "\n",
    "        # Early Stopping\n",
    "        if macro_f1 > best_f1 + es_min_delta:\n",
    "            best_f1 = macro_f1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            if ema:\n",
    "                best_ema_shadow = {k: v.detach().cpu().clone() for k, v in ema.shadow.items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if early_stop and no_improve >= es_patience:\n",
    "                print(f\"[EarlyStop] нет улучшений {es_patience} эпох. Лучший val_macro_f1={best_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    # Откат на лучшие веса\n",
    "    model.load_state_dict(best_state)\n",
    "    if ema and best_ema_shadow is not None:\n",
    "        ema.shadow = best_ema_shadow\n",
    "    if use_swa:\n",
    "        update_bn(train_loader, swa_model, device=device)\n",
    "        model = swa_model\n",
    "\n",
    "    # Финальный отчёт по валидации\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pr = model(X).argmax(-1)\n",
    "            y_pred += pr.cpu().tolist()\n",
    "            y_true += y.cpu().tolist()\n",
    "    rep = classification_report(y_true, y_pred, digits=4, zero_division=0, output_dict=True)\n",
    "    cm = confusion_matrix(y_true, y_pred).tolist()\n",
    "    metrics_blob = {\"history\": history,\n",
    "                    \"classification_report\": rep,\n",
    "                    \"confusion_matrix\": cm,\n",
    "                    \"best_val_macro_f1\": best_f1}\n",
    "    (EXPERIMENTS_DIR/\"last_val_metrics.json\").write_text(json.dumps(metrics_blob, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(\"[trainer] Saved val metrics → experiments/last_val_metrics.json\")\n",
    "    return model, metrics_blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b5aa0",
   "metadata": {},
   "source": [
    "## 8) TTA и bias tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fb55c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL_RE   = re.compile(r'https?://\\S+')\n",
    "EMOJI_RE = re.compile(r'[\\U0001F300-\\U0001FAFF]+')\n",
    "\n",
    "def tta_variants(text: str) -> List[str]:\n",
    "    t = text if isinstance(text, str) else str(text)\n",
    "    cand = [t, t.lower(), URL_RE.sub(\"<URL>\", t), EMOJI_RE.sub(\"<EMOJI>\", t), re.sub(r\"[^\\w\\s]\", \" \", t.lower())]\n",
    "    seen, out = set(), []\n",
    "    for v in cand:\n",
    "        if v not in seen:\n",
    "            out.append(v); seen.add(v)\n",
    "    return out[:7]\n",
    "\n",
    "def predict_proba_tta(model, texts: List[str], ft_model, max_tokens: int, batch=256):\n",
    "    all_probs = []\n",
    "    for txt in texts:\n",
    "        vs = tta_variants(txt)\n",
    "        Xv = [embed_tokens(_tokenize(v), ft_model, max_tokens) for v in vs]\n",
    "        Xv = torch.stack(Xv, dim=0)\n",
    "        dl = DataLoader([(x, 0) for x in Xv], batch_size=batch, shuffle=False)\n",
    "        model.eval()\n",
    "        probs_acc = None\n",
    "        with torch.no_grad():\n",
    "            for X,_ in dl:\n",
    "                X = X.to(device)\n",
    "                logits = model(X)\n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "                probs_acc = probs if probs_acc is None else np.vstack((probs_acc, probs))\n",
    "        all_probs.append(probs_acc.mean(axis=0))\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "def tune_logit_biases(probs_val: np.ndarray, y_val: List[int], grid=(-0.5, -0.25, 0.0, 0.25, 0.5)):\n",
    "    from itertools import product\n",
    "    C = probs_val.shape[1]\n",
    "    best_f1, best_b = -1.0, np.zeros(C, dtype=np.float32)\n",
    "    for deltas in product(grid, repeat=C):\n",
    "        shifted = probs_val + np.array(deltas)[None, :]\n",
    "        pred = shifted.argmax(1)\n",
    "        f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_b = f1, np.array(deltas, dtype=np.float32)\n",
    "    return best_b, best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca559bd",
   "metadata": {},
   "source": [
    "## 9) Чтение прошлых результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e62357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_previous_runs(exp_dir: Path):\n",
    "    runs = []\n",
    "    for p in [exp_dir/\"runs.jsonl\", Path(\"runs.jsonl\")]:\n",
    "        if p.exists():\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        runs.append(json.loads(line))\n",
    "                    except:\n",
    "                        pass\n",
    "    return runs\n",
    "\n",
    "def load_best_run(exp_dir: Path):\n",
    "    for p in [exp_dir/\"best_run.json\", Path(\"best_run.json\")]:\n",
    "        if p.exists():\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    return None\n",
    "\n",
    "def get_params_for_id(runs: List[Dict[str,Any]], run_id: str) -> Optional[Dict[str,Any]]:\n",
    "    for r in runs:\n",
    "        if isinstance(r, dict):\n",
    "            pp = r.get(\"parameters\")\n",
    "            if isinstance(pp, dict) and pp.get(\"id\") == run_id:\n",
    "                return pp\n",
    "            if r.get(\"id\") == run_id and \"parameters\" in r:\n",
    "                return r[\"parameters\"]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c59cb",
   "metadata": {},
   "source": [
    "## 10) Оркестратор improve_from_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b26310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === improve_from_best: форсим базу = ADV_DEFAULT; опц. свип LR; ансамбль по сид-ам ===\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def improve_from_best(seeds: List[int] = [42,1337,2025],\n",
    "                      save_name: str = \"ensemble_95_target.json\",\n",
    "                      lr_override: Optional[float] = None,\n",
    "                      lr_grid: Optional[List[float]] = None,\n",
    "                      lr_grid_epochs: Optional[int] = None):\n",
    "    # 1) берём пресет как БАЗУ\n",
    "    data_p  = dict(ADV_DEFAULT[\"data\"])\n",
    "    model_p = dict(ADV_DEFAULT[\"model\"])\n",
    "    train_p = dict(ADV_DEFAULT[\"train\"])\n",
    "    class_labels = data_p[\"class_labels\"]\n",
    "\n",
    "    # 2) читаем датасет и сплитим\n",
    "    df = pd.read_csv(data_p[\"dataset_path\"])\n",
    "    texts_all = df[\"text\"].astype(str).tolist()\n",
    "    y_all = np.argmax(df[class_labels].values, axis=1).tolist()\n",
    "    rs = int(data_p.get(\"random_state\", 42))\n",
    "    test_size = float(data_p.get(\"test_size\", 0.2))\n",
    "    val_size  = float(data_p.get(\"val_size\", 0.1))\n",
    "    x_tmp, x_test, y_tmp, y_test = train_test_split(texts_all, y_all, test_size=test_size, random_state=rs, stratify=y_all)\n",
    "    rel_val = val_size / (1.0 - test_size)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_tmp, y_tmp, test_size=rel_val, random_state=rs, stratify=y_tmp)\n",
    "\n",
    "    # 3) fastText и стриминговые датасеты\n",
    "    ft = ensure_fasttext_model(str(FASTTEXT_BIN))\n",
    "    max_tokens = int(data_p[\"max_tokens\"])\n",
    "    train_ds = StreamingTensorDataset(x_train, y_train, ft, max_tokens)\n",
    "    val_ds   = StreamingTensorDataset(x_val,   y_val,   ft, max_tokens)\n",
    "    test_ds  = StreamingTensorDataset(x_test,  y_test,  ft, max_tokens)\n",
    "    num_classes = len(class_labels)\n",
    "\n",
    "    # helper — один прогон\n",
    "    def _train_and_probs(seed: int, base_lr: float, epochs_use: int):\n",
    "        set_seed(int(seed))\n",
    "        model = make_model_from_params(model_p, num_classes=num_classes, embed_dim=model_p.get(\"embed_dim\",300))\n",
    "        model, _ = train_model_v3(\n",
    "            model, train_ds, val_ds,\n",
    "            epochs=train_p.get(\"epochs\", 30),           # ← без min(20,...)\n",
    "            base_lr=best_lr,\n",
    "            weight_decay=train_p.get(\"weight_decay\", 5e-5),\n",
    "            pct_start=train_p.get(\"pct_start\", 0.05),\n",
    "            use_onecycle=(train_p.get(\"scheduler\",\"onecycle\") == \"onecycle\"),\n",
    "            use_ema=train_p.get(\"use_ema\", True),\n",
    "            use_swa=train_p.get(\"use_swa\", True),       # ← включили SWA\n",
    "            loss_type=\"focal\",\n",
    "            focal_gamma=train_p.get(\"focal_gamma\", 1.6),\n",
    "            label_smoothing=train_p.get(\"label_smoothing\", 0.06),\n",
    "            warmup_ce_epochs=6,                         # ← подольше прогрев CE\n",
    "            batch_size=min(36, train_p.get(\"batch_size\", 36)),\n",
    "            num_workers=0,\n",
    "            ms_dropout_samples=model_p.get(\"ms_dropout_samples\", 6),\n",
    "            class_names=class_labels,\n",
    "            early_stop=True, es_patience=6, es_min_delta=5e-4\n",
    "        )\n",
    "\n",
    "        pv = predict_proba_tta(model, x_val,  ft, max_tokens)\n",
    "        pt = predict_proba_tta(model, x_test, ft, max_tokens)\n",
    "        return pv, pt\n",
    "\n",
    "    # 4) выбор LR (по желанию)\n",
    "    if lr_override is not None:\n",
    "        best_lr = float(lr_override)\n",
    "        print(f\"[LR] override = {best_lr}\")\n",
    "    elif lr_grid:\n",
    "        probe_seed = int(seeds[0]) if seeds else 42\n",
    "        probe_epochs = int(lr_grid_epochs or min(14, train_p[\"epochs\"]))\n",
    "        print(f\"[LR] grid search on seed={probe_seed}, epochs={probe_epochs}: {lr_grid}\")\n",
    "        best_lr, best_f1 = None, -1.0\n",
    "        for lr in lr_grid:\n",
    "            pv, _ = _train_and_probs(probe_seed, float(lr), probe_epochs)\n",
    "            bias,_ = tune_logit_biases(pv, y_val, grid=(-0.25,-0.1,0.0,0.1,0.25))\n",
    "            f1 = f1_score(y_val, (pv + bias[None,:]).argmax(1), average=\"macro\")\n",
    "            print(f\"  lr={lr:.6f}  val_macro_f1={f1:.4f}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_lr = f1, float(lr)\n",
    "        print(f\"[LR] best = {best_lr:.6f} (val_macro_f1={best_f1:.4f})\")\n",
    "    else:\n",
    "        best_lr = float(train_p[\"learning_rate\"])\n",
    "        print(f\"[LR] from preset = {best_lr}\")\n",
    "\n",
    "    # 5) ансамбль по сид-ам\n",
    "    probs_val_sum = probs_test_sum = None\n",
    "    for s in seeds:\n",
    "        pv, pt = _train_and_probs(int(s), best_lr, epochs_use=min(20, train_p[\"epochs\"]))\n",
    "        probs_val_sum  = pv if probs_val_sum  is None else (probs_val_sum  + pv)\n",
    "        probs_test_sum = pt if probs_test_sum is None else (probs_test_sum + pt)\n",
    "\n",
    "    probs_val_mean  = probs_val_sum  / max(1, len(seeds))\n",
    "    probs_test_mean = probs_test_sum / max(1, len(seeds))\n",
    "\n",
    "    bias, _ = tune_logit_biases(probs_val_mean, y_val,\n",
    "                                grid=(-0.25, -0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.25))\n",
    "    val_pred  = (probs_val_mean  + bias[None,:]).argmax(1)\n",
    "    test_pred = (probs_test_mean + bias[None,:]).argmax(1)\n",
    "\n",
    "    val_f1   = f1_score(y_val,  val_pred,  average=\"macro\")\n",
    "    test_f1  = f1_score(y_test, test_pred, average=\"macro\")\n",
    "    val_acc  = accuracy_score(y_val,  val_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "    out = {\n",
    "        \"base_id\": \"auto_advanced_medium\",\n",
    "        \"seeds\": list(map(int, seeds)),\n",
    "        \"val_macro_f1\": float(val_f1),\n",
    "        \"val_accuracy\": float(val_acc),\n",
    "        \"test_macro_f1\": float(test_f1),\n",
    "        \"test_accuracy\": float(test_acc),\n",
    "        \"bias\": bias.tolist(),\n",
    "        \"used_params\": {\"data\":data_p, \"model\":model_p, \"train\":train_p},\n",
    "        \"chosen_lr\": best_lr,\n",
    "        \"notes\": f\"Advanced preset + Ensemble x{len(seeds)} + TTA + bias (num_workers=0)\"\n",
    "    }\n",
    "    (EXPERIMENTS_DIR/save_name).write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[RESULT] VAL F1={val_f1:.4f}  ACC={val_acc:.4f} | TEST F1={test_f1:.4f}  ACC={test_acc:.4f} | LR={best_lr}\")\n",
    "    print(f\"[SAVED] {EXPERIMENTS_DIR/save_name}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3c082",
   "metadata": {},
   "source": [
    "## 11) Запуск улучшения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b1d3e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FT] Using native fasttext (subword).\n",
      "[LR] override = 0.0008\n",
      "[Seed] set to 42\n",
      "[Loaders] batch_size=36, num_workers=0, pin_memory=False\n",
      "[Epoch 1/30] lr=6.08e-04  loss_type=CE  train_loss=0.4666  val_macro_f1=0.6506\n",
      "[Epoch 2/30] lr=7.99e-04  loss_type=CE  train_loss=0.2943  val_macro_f1=0.7614\n",
      "[Epoch 3/30] lr=7.95e-04  loss_type=CE  train_loss=0.2361  val_macro_f1=0.7890\n",
      "[Epoch 4/30] lr=7.85e-04  loss_type=CE  train_loss=0.2057  val_macro_f1=0.8098\n",
      "[Epoch 5/30] lr=7.71e-04  loss_type=CE  train_loss=0.1874  val_macro_f1=0.8263\n",
      "[Epoch 6/30] lr=7.52e-04  loss_type=CE  train_loss=0.1786  val_macro_f1=0.8407\n",
      "[Epoch 7/30] lr=7.29e-04  loss_type=Focal  train_loss=0.0126  val_macro_f1=0.8483\n",
      "[Epoch 8/30] lr=7.02e-04  loss_type=Focal  train_loss=0.0106  val_macro_f1=0.8597\n",
      "[Epoch 9/30] lr=6.71e-04  loss_type=Focal  train_loss=0.0078  val_macro_f1=0.8634\n",
      "[Epoch 10/30] lr=6.37e-04  loss_type=Focal  train_loss=0.0064  val_macro_f1=0.8674\n",
      "[Epoch 11/30] lr=6.00e-04  loss_type=Focal  train_loss=0.0056  val_macro_f1=0.8766\n",
      "[Epoch 12/30] lr=5.61e-04  loss_type=Focal  train_loss=0.0053  val_macro_f1=0.8793\n",
      "[Epoch 13/30] lr=5.19e-04  loss_type=Focal  train_loss=0.0041  val_macro_f1=0.8817\n",
      "[Epoch 14/30] lr=4.77e-04  loss_type=Focal  train_loss=0.0037  val_macro_f1=0.8833\n",
      "[Epoch 15/30] lr=4.33e-04  loss_type=Focal  train_loss=0.0031  val_macro_f1=0.8912\n",
      "[Epoch 16/30] lr=3.89e-04  loss_type=Focal  train_loss=0.0028  val_macro_f1=0.8918\n",
      "[Epoch 17/30] lr=3.45e-04  loss_type=Focal  train_loss=0.0022  val_macro_f1=0.8951\n",
      "[Epoch 18/30] lr=3.02e-04  loss_type=Focal  train_loss=0.0019  val_macro_f1=0.8989\n",
      "[Epoch 19/30] lr=2.60e-04  loss_type=Focal  train_loss=0.0017  val_macro_f1=0.9015\n",
      "[Epoch 20/30] lr=2.19e-04  loss_type=Focal  train_loss=0.0014  val_macro_f1=0.9023\n",
      "[Epoch 21/30] lr=1.81e-04  loss_type=Focal  train_loss=0.0012  val_macro_f1=0.9021\n",
      "[Epoch 22/30] lr=1.46e-04  loss_type=Focal  train_loss=0.0011  val_macro_f1=0.9044\n",
      "[Epoch 23/30] lr=1.13e-04  loss_type=Focal  train_loss=0.0009  val_macro_f1=0.9045\n",
      "[Epoch 24/30] lr=8.43e-05  loss_type=Focal  train_loss=0.0008  val_macro_f1=0.9059\n",
      "[Epoch 25/30] lr=5.92e-05  loss_type=Focal  train_loss=0.0008  val_macro_f1=0.9070\n",
      "[Epoch 26/30] lr=3.83e-05  loss_type=Focal  train_loss=0.0007  val_macro_f1=0.9058\n",
      "[Epoch 27/30] lr=2.17e-05  loss_type=Focal  train_loss=0.0006  val_macro_f1=0.9074\n",
      "[Epoch 28/30] lr=9.68e-06  loss_type=Focal  train_loss=0.0006  val_macro_f1=0.9076\n",
      "[Epoch 29/30] lr=2.43e-06  loss_type=Focal  train_loss=0.0005  val_macro_f1=0.9077\n",
      "[Epoch 30/30] lr=3.20e-09  loss_type=Focal  train_loss=0.0005  val_macro_f1=0.9080\n",
      "[trainer] Saved val metrics → experiments/last_val_metrics.json\n",
      "[RESULT] VAL F1=0.9100  ACC=0.9649 | TEST F1=0.8989  ACC=0.9611 | LR=0.0008\n",
      "[SAVED] e:\\Python_projects\\Toxic_class_base\\experiments\\ensemble_95_target.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'base_id': 'auto_advanced_medium',\n",
       " 'seeds': [42],\n",
       " 'val_macro_f1': 0.9099517290073056,\n",
       " 'val_accuracy': 0.9649200531636393,\n",
       " 'test_macro_f1': 0.8989290109492295,\n",
       " 'test_accuracy': 0.9610737444117765,\n",
       " 'bias': [0.0, -0.25, -0.25],\n",
       " 'used_params': {'data': {'sample_per_class': 16000,\n",
       "   'max_tokens': 160,\n",
       "   'test_size': 0.2,\n",
       "   'val_size': 0.1,\n",
       "   'random_state': 42,\n",
       "   'class_labels': ['neutral', 'toxic_not_threat', 'toxic_threat'],\n",
       "   'dataset_path': 'e:\\\\Python_projects\\\\Toxic_class_base\\\\rus_toxic_full_df.csv'},\n",
       "  'model': {'embed_dim': 300,\n",
       "   'hidden_size': 320,\n",
       "   'num_layers': 2,\n",
       "   'dropout': 0.28,\n",
       "   'use_advanced_model': True,\n",
       "   'use_advanced_attention': True,\n",
       "   'ms_dropout_samples': 6},\n",
       "  'train': {'batch_size': 36,\n",
       "   'epochs': 30,\n",
       "   'learning_rate': 0.0008,\n",
       "   'weight_decay': 5e-05,\n",
       "   'scheduler': 'onecycle',\n",
       "   'pct_start': 0.05,\n",
       "   'use_ema': True,\n",
       "   'use_swa': True,\n",
       "   'focal_gamma': 1.6,\n",
       "   'label_smoothing': 0.06}},\n",
       " 'chosen_lr': 0.0008,\n",
       " 'notes': 'Advanced preset + Ensemble x1 + TTA + bias (num_workers=0)'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = improve_from_best(seeds=[42], lr_override=8e-4)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf67d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Итог для отчёта ===\n",
      "Test accuracy (главный критерий задания): 0.9611\n",
      "Test macro-F1: 0.8989\n",
      "Val macro-F1 (для подбора модели): 0.9100\n",
      "Val accuracy: 0.9649\n",
      "\n",
      "— Per-class на валидации (macro-F1 мы использовали для выбора):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.987766</td>\n",
       "      <td>0.979134</td>\n",
       "      <td>0.983431</td>\n",
       "      <td>20368.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857809</td>\n",
       "      <td>0.903622</td>\n",
       "      <td>0.880120</td>\n",
       "      <td>3258.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.856669</td>\n",
       "      <td>0.859518</td>\n",
       "      <td>0.858091</td>\n",
       "      <td>1203.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.963430</td>\n",
       "      <td>0.963430</td>\n",
       "      <td>0.963430</td>\n",
       "      <td>0.96343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.900748</td>\n",
       "      <td>0.914091</td>\n",
       "      <td>0.907214</td>\n",
       "      <td>24829.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.963430</td>\n",
       "      <td>0.963802</td>\n",
       "      <td>24829.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.987766  0.979134  0.983431  20368.00000\n",
       "1              0.857809  0.903622  0.880120   3258.00000\n",
       "2              0.856669  0.859518  0.858091   1203.00000\n",
       "accuracy       0.963430  0.963430  0.963430      0.96343\n",
       "macro avg      0.900748  0.914091  0.907214  24829.00000\n",
       "weighted avg   0.964362  0.963430  0.963802  24829.00000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Confusion matrix (валидация):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:neutral</th>\n",
       "      <th>pred:not_threat</th>\n",
       "      <th>pred:threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true:neutral</th>\n",
       "      <td>19943</td>\n",
       "      <td>361</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:not_threat</th>\n",
       "      <td>205</td>\n",
       "      <td>2944</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:threat</th>\n",
       "      <td>42</td>\n",
       "      <td>127</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred:neutral  pred:not_threat  pred:threat\n",
       "true:neutral            19943              361           64\n",
       "true:not_threat           205             2944          109\n",
       "true:threat                42              127         1034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Saved] report/summary_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# === Итоговые метрики для отчёта ===\n",
    "import json, os, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "EXP = Path(\"experiments\")\n",
    "res = globals().get(\"res\", None)  # возьмём из последнего вызова improve_from_best\n",
    "\n",
    "val_f1 = test_f1 = val_acc = test_acc = None\n",
    "if isinstance(res, dict):\n",
    "    val_f1  = float(res.get(\"val_macro_f1\", np.nan))\n",
    "    test_f1 = float(res.get(\"test_macro_f1\", np.nan))\n",
    "    val_acc = float(res.get(\"val_accuracy\", np.nan))\n",
    "    test_acc= float(res.get(\"test_accuracy\", np.nan))\n",
    "\n",
    "if (val_f1 is None or np.isnan(val_f1)) and (EXP/\"last_val_metrics.json\").exists():\n",
    "    j = json.loads((EXP/\"last_val_metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "    val_f1 = float(j.get(\"best_val_macro_f1\", np.nan))\n",
    "if (test_f1 is None or np.isnan(test_f1)) and (EXP/\"ensemble_95_target.json\").exists():\n",
    "    j = json.loads((EXP/\"ensemble_95_target.json\").read_text(encoding=\"utf-8\"))\n",
    "    test_f1  = float(j.get(\"test_macro_f1\", np.nan))\n",
    "    test_acc = float(j.get(\"test_accuracy\", np.nan))\n",
    "    val_acc  = float(j.get(\"val_accuracy\", np.nan)) if val_acc is None else val_acc\n",
    "\n",
    "# Печать короткого сводного блока (то, что нужно в отчёт)\n",
    "print(\"=== Итог для отчёта ===\")\n",
    "print(f\"Test accuracy (главный критерий задания): {test_acc:.4f}\" if test_acc is not None else \"Test accuracy: n/a\")\n",
    "print(f\"Test macro-F1: {test_f1:.4f}\" if test_f1 is not None else \"Test macro-F1: n/a\")\n",
    "print(f\"Val macro-F1 (для подбора модели): {val_f1:.4f}\" if val_f1 is not None else \"Val macro-F1: n/a\")\n",
    "print(f\"Val accuracy: {val_acc:.4f}\" if val_acc is not None else \"Val accuracy: n/a\")\n",
    "\n",
    "# Если есть подробный classification_report/CM из последнего валидационного прогона\n",
    "report_path = EXP/\"last_val_metrics.json\"\n",
    "if report_path.exists():\n",
    "    j = json.loads(report_path.read_text(encoding=\"utf-8\"))\n",
    "    cr = j.get(\"classification_report\")\n",
    "    cm = j.get(\"confusion_matrix\")\n",
    "    if cr:\n",
    "        print(\"\\n— Per-class на валидации (macro-F1 мы использовали для выбора):\")\n",
    "        df_cr = pd.DataFrame(cr).T\n",
    "        display(df_cr)\n",
    "    if cm:\n",
    "        print(\"— Confusion matrix (валидация):\")\n",
    "        df_cm = pd.DataFrame(cm,\n",
    "                             index=[\"true:neutral\",\"true:not_threat\",\"true:threat\"],\n",
    "                             columns=[\"pred:neutral\",\"pred:not_threat\",\"pred:threat\"])\n",
    "        display(df_cm)\n",
    "\n",
    "# Сохраним мини-выжимку для вставки в отчёт\n",
    "REPORT = Path(\"report\"); REPORT.mkdir(exist_ok=True)\n",
    "summary = {\n",
    "    \"test_accuracy\": test_acc,\n",
    "    \"test_macro_f1\": test_f1,\n",
    "    \"val_macro_f1\": val_f1,\n",
    "    \"val_accuracy\": val_acc,\n",
    "}\n",
    "(Path(REPORT/\"summary_metrics.json\")).write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"\\n[Saved] report/summary_metrics.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
