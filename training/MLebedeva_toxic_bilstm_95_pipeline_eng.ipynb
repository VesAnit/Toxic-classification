{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toxic BiLSTM — 95% Target Pipeline (structured)\n",
        "\n",
        "Этот ноутбук:\n",
        "1. Находит пути к `cc.ru.300.bin`, `rus_toxic_full_df.csv`, папке `experiments/`.\n",
        "2. Подтягивает **лучший прошлый запуск** и тренирует ансамбль поверх него.\n",
        "3. Загружает FastText через **gensim** (без нативной сборки fasttext, дружит с Windows).\n",
        "4. Использует **StreamingTensorDataset** (эмбеддинги на лету, без OOM).\n",
        "5. Делает **TTA + bias-tuning**, сохраняет итог в `experiments/ensemble_95_target_eng.json`,\n",
        "   обновляет `best_run_eng.json`, если стало лучше.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Жёсткая привязка путей (исправь BASE, если нужно)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE            : E:\\Python_projects\\Toxic_class_base\n",
            "cc.en.300.bin   : e:\\Python_projects\\Toxic_class_base\\cc.en.300.bin exists: True\n",
            "dataset csv     : e:\\Python_projects\\Toxic_class_base\\english_subset_50.csv exists: True\n",
            "experiments dir : e:\\Python_projects\\Toxic_class_base\\experiments_en exists: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "BASE = Path.cwd()  # Можно поставить, например: Path(r\"e:\\Python_projects\\Toxic_class_base\")\n",
        "\n",
        "FASTTEXT_BIN   = (BASE / \"cc.en.300.bin\")\n",
        "DATASET_CSV    = (BASE / \"english_subset_50.csv\")\n",
        "EXPERIMENTS_DIR = (BASE / \"experiments_en\")\n",
        "\n",
        "print(\"BASE            :\", BASE.resolve())\n",
        "print(\"cc.en.300.bin   :\", FASTTEXT_BIN, \"exists:\", FASTTEXT_BIN.exists())\n",
        "print(\"dataset csv     :\", DATASET_CSV, \"exists:\", DATASET_CSV.exists())\n",
        "print(\"experiments dir :\", EXPERIMENTS_DIR, \"exists:\", EXPERIMENTS_DIR.exists())\n",
        "\n",
        "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Импорты, сиды, предупреждения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Init] device: cuda\n",
            "[Seed] set to 42\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re, math, inspect, json, warnings\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[Init] device:\", device)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"RNN module weights are not part of single contiguous chunk\")\n",
        "\n",
        "GLOBAL_RANDOM_SEED = 42\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"[Seed] set to {seed}\")\n",
        "\n",
        "set_seed(GLOBAL_RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) FastText через gensim (совместимо с версиями с/без `mmap`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from gensim.models.fasttext import load_facebook_vectors  # pip install gensim\n",
        "\n",
        "def _hash_vec(word: str, dim: int, seed: int = 1337):\n",
        "    rs = np.random.RandomState(abs(hash((word, seed))) % (2**32))\n",
        "    v = rs.normal(0, 1, size=(dim,)).astype(np.float32)\n",
        "    v /= (np.linalg.norm(v) + 1e-8)\n",
        "    return v\n",
        "\n",
        "class GensimFTWrapper:\n",
        "    def __init__(self, kv):\n",
        "        self.kv = kv\n",
        "        self._dim = int(kv.vector_size)\n",
        "    def get_dimension(self):\n",
        "        return self._dim\n",
        "    def get_word_vector(self, word: str):\n",
        "        try:\n",
        "            return self.kv.get_vector(word)\n",
        "        except KeyError:\n",
        "            return _hash_vec(word, self._dim)\n",
        "\n",
        "# --- fastText loader: native -> gensim subword -> gensim vectors ---\n",
        "import os, numpy as np\n",
        "from functools import lru_cache\n",
        "\n",
        "def _hash_vec(word: str, dim: int) -> np.ndarray:\n",
        "    h = (hash(word) & 0xFFFFFFFF)\n",
        "    rng = np.random.default_rng(h)\n",
        "    v = rng.normal(0.0, 0.5, size=(dim,)).astype(np.float32)\n",
        "    n = float(np.linalg.norm(v)) + 1e-9\n",
        "    return (v / n).astype(np.float32)\n",
        "\n",
        "class _NativeFTWrapper:\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self._dim = int(m.get_dimension())\n",
        "    def get_dimension(self): return self._dim\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def get_word_vector(self, w: str) -> np.ndarray:\n",
        "        return np.asarray(self.m.get_word_vector(w), dtype=np.float32)\n",
        "\n",
        "class _GensimSubwordWrapper:\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self._dim = int(m.wv.vector_size)\n",
        "    def get_dimension(self): return self._dim\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def get_word_vector(self, w: str) -> np.ndarray:\n",
        "        # у gensim FastText (model.wv) OOV собираются по n-gram, как в native\n",
        "        return self.m.wv.get_vector(w, norm=False).astype(np.float32)\n",
        "\n",
        "class _GensimVectorsWrapper:\n",
        "    def __init__(self, kv):\n",
        "        self.kv = kv\n",
        "        self._dim = int(kv.vector_size)\n",
        "    def get_dimension(self): return self._dim\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def get_word_vector(self, w: str) -> np.ndarray:\n",
        "        try:\n",
        "            return self.kv.get_vector(w).astype(np.float32)\n",
        "        except KeyError:\n",
        "            return _hash_vec(w, self._dim)  # fallback для OOV\n",
        "\n",
        "def ensure_fasttext_model(model_path: str, download_if_missing: bool = False):\n",
        "    if not (isinstance(model_path, str) and os.path.isfile(model_path)):\n",
        "        raise FileNotFoundError(f\"FastText model not found: {model_path}\")\n",
        "\n",
        "    # 1) native fasttext (лучший вариант)\n",
        "    try:\n",
        "        import fasttext\n",
        "        m = fasttext.load_model(model_path)\n",
        "        print(\"[FT] Using native fasttext (subword).\")\n",
        "        return _NativeFTWrapper(m)\n",
        "    except Exception as e:\n",
        "        print(f\"[FT] native fasttext unavailable: {e}\")\n",
        "\n",
        "    # 2) gensim subword model\n",
        "    try:\n",
        "        from gensim.models.fasttext import load_facebook_model\n",
        "        m = load_facebook_model(model_path)\n",
        "        print(\"[FT] Using gensim FastText MODEL (subword).\")\n",
        "        return _GensimSubwordWrapper(m)\n",
        "    except Exception as e:\n",
        "        print(f\"[FT] gensim subword model unavailable: {e}\")\n",
        "\n",
        "    # 3) fallback: plain vectors (без subword)\n",
        "    from gensim.models.fasttext import load_facebook_vectors\n",
        "    kv = load_facebook_vectors(model_path)\n",
        "    print(\"[FT] Using gensim word VECTORS (NO subword) — expect lower quality.\")\n",
        "    return _GensimVectorsWrapper(kv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Токенизация и стриминговый датасет (без OOM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "TOKEN_RE = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
        "\n",
        "def _tokenize(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return TOKEN_RE.findall(text.lower())\n",
        "\n",
        "def embed_tokens(tokens: List[str], ft_model, max_tokens: int) -> torch.Tensor:\n",
        "    toks = tokens[:max_tokens]\n",
        "    dim = int(ft_model.get_dimension())\n",
        "    X = np.zeros((max_tokens, dim), dtype=np.float32)\n",
        "    for j, tok in enumerate(toks):\n",
        "        X[j, :] = ft_model.get_word_vector(tok)\n",
        "    return torch.from_numpy(X)\n",
        "\n",
        "class StreamingTensorDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], y_list: List[int], ft_model, max_tokens: int):\n",
        "        self.texts = list(texts)\n",
        "        self.y = torch.tensor(list(y_list), dtype=torch.long)\n",
        "        self.ft = ft_model\n",
        "        self.max_tokens = int(max_tokens)\n",
        "        self.tensors = (None, self.y)  # for WeightedRandomSampler\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        toks = _tokenize(self.texts[idx])\n",
        "        x = embed_tokens(toks, self.ft, self.max_tokens)\n",
        "        y = self.y[idx]\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Модель BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embed_dim: int, hidden_size: int, num_layers: int, dropout: float, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self.lstm, \"flatten_parameters\"):\n",
        "            self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x)     # (B,T,2H)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "def make_bilstm(num_classes: int, embed_dim: int, hidden_size: int, num_layers: int, dropout: float):\n",
        "    return BiLSTM(embed_dim=embed_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, num_classes=num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "620b6d96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === BiLSTM_Advanced с простым self-attention ===\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class BiLSTM_Advanced(nn.Module):\n",
        "    def __init__(self, embed_dim: int, hidden_size: int, num_layers: int,\n",
        "                 dropout: float, num_classes: int, use_advanced_attention: bool = True):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
        "            bidirectional=True, dropout=(dropout if num_layers > 1 else 0.0), batch_first=True\n",
        "        )\n",
        "        self.use_attn = use_advanced_attention\n",
        "        if self.use_attn:\n",
        "            self.attn = nn.Sequential(\n",
        "                nn.Linear(2*hidden_size, 2*hidden_size),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(2*hidden_size, 1)\n",
        "            )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(2*hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self.lstm, \"flatten_parameters\"):\n",
        "            self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x)                      # (B, T, 2H)\n",
        "        if self.use_attn:\n",
        "            e = self.attn(out).squeeze(-1)         # (B, T)\n",
        "            a = torch.softmax(e, dim=1).unsqueeze(-1)\n",
        "            h = (out * a).sum(dim=1)               # (B, 2H)\n",
        "        else:\n",
        "            h = out[:, -1, :]\n",
        "        h = self.dropout(h)\n",
        "        return self.fc(h)\n",
        "\n",
        "# фабрика модели: выбирает advanced по флагу\n",
        "def make_model_from_params(model_p: dict, num_classes: int, embed_dim: int):\n",
        "    if model_p.get(\"use_advanced_model\", False):\n",
        "        return BiLSTM_Advanced(\n",
        "            embed_dim=embed_dim,\n",
        "            hidden_size=model_p.get(\"hidden_size\", 320),\n",
        "            num_layers=model_p.get(\"num_layers\", 2),\n",
        "            dropout=model_p.get(\"dropout\", 0.28),\n",
        "            num_classes=num_classes,\n",
        "            use_advanced_attention=model_p.get(\"use_advanced_attention\", True),\n",
        "        )\n",
        "    # fallback на классический BiLSTM, если нужен\n",
        "    return BiLSTM(\n",
        "        embed_dim=embed_dim,\n",
        "        hidden_size=model_p.get(\"hidden_size\", 320),\n",
        "        num_layers=model_p.get(\"num_layers\", 2),\n",
        "        dropout=model_p.get(\"dropout\", 0.28),\n",
        "        num_classes=num_classes,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "577abf40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === DEFAULT: auto_advanced_medium (усилен под финал) ===\n",
        "ADV_DEFAULT = {\n",
        "    \"data\": {\n",
        "        \"sample_per_class\": 16000, # попробовать 17000\n",
        "        \"max_tokens\": 160, # попробовать 170\n",
        "        \"test_size\": 0.20,\n",
        "        \"val_size\": 0.10,\n",
        "        \"random_state\": 42,\n",
        "        \"class_labels\": [\"neutral\",\"toxic_not_threat\",\"toxic_threat\"],\n",
        "        \"dataset_path\": str(DATASET_CSV),\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"embed_dim\": 300,\n",
        "        \"hidden_size\": 320,\n",
        "        \"num_layers\": 2,\n",
        "        \"dropout\": 0.28,\n",
        "        \"use_advanced_model\": True,\n",
        "        \"use_advanced_attention\": True,\n",
        "        \"ms_dropout_samples\": 6,   # было 5, попробовать 7\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"batch_size\": 36,          # поднимай до 40, если VRAM позволяет\n",
        "        \"epochs\": 30,              # было 24\n",
        "        \"learning_rate\": 8e-4,     # лучший из свипа\n",
        "        \"weight_decay\": 5e-5,\n",
        "        \"scheduler\": \"onecycle\",\n",
        "        \"pct_start\": 0.05,         # было 0.10 (раньше пик OneCycle)\n",
        "        \"use_ema\": True,\n",
        "        \"use_swa\": True,           # включаем SWA\n",
        "        \"focal_gamma\": 1.6,        # было 1.5\n",
        "        \"label_smoothing\": 0.06,   # было 0.05\n",
        "    }\n",
        "}\n",
        "\n",
        "ADV_DEFAULT[\"data\"][\"take_fraction\"] = 1.0  # при отладке можно 0.5–0.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Потери, EMA, семплер, лоадеры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def make_class_alpha(train_ds, num_classes: int) -> torch.Tensor:\n",
        "    y = train_ds.tensors[1].cpu().tolist()\n",
        "    cnt = Counter(y)\n",
        "    # инверсия частот\n",
        "    w = [1.0 / max(1, cnt.get(i, 1)) for i in range(num_classes)]\n",
        "    w = torch.tensor(w, dtype=torch.float32)\n",
        "    # нормализация: среднее = 1.0 (стабильнее для оптимизатора)\n",
        "    w = w * (num_classes / w.sum().clamp_min(1e-8))\n",
        "    return w\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma: float = 1.5, alpha: torch.Tensor | None = None, reduction: str = \"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        # храним alpha как buffer (float32); может быть на CPU — перенесём в forward\n",
        "        if isinstance(alpha, torch.Tensor):\n",
        "            self.register_buffer(\"alpha\", alpha.float())\n",
        "        else:\n",
        "            self.alpha = None\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        logp = F.log_softmax(logits, dim=-1)                 \n",
        "        p    = logp.exp()\n",
        "        logpt = logp.gather(1, target.unsqueeze(1)).squeeze(1)  \n",
        "        pt    = p.gather(1, target.unsqueeze(1)).squeeze(1)     \n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha\n",
        "            if alpha.device != target.device:                \n",
        "                alpha = alpha.to(target.device)              \n",
        "            at = alpha[target]                               \n",
        "        else:\n",
        "            at = 1.0\n",
        "\n",
        "        loss = - at * ((1.0 - pt).clamp_min(1e-8) ** self.gamma) * logpt\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        if self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow, self.backup = {}, {}\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n] = p.data.clone()\n",
        "    def update(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n] = (1 - self.decay) * p.data + self.decay * self.shadow[n]\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup = {}\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[n] = p.data.clone()\n",
        "                p.data = self.shadow[n]\n",
        "    def restore(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad and n in self.backup:\n",
        "                p.data = self.backup[n]\n",
        "        self.backup = {}\n",
        "\n",
        "IS_WIN = os.name == \"nt\"\n",
        "\n",
        "def sampler_from_ds(train_ds):\n",
        "    y = train_ds.tensors[1].cpu().tolist()\n",
        "    cnt = Counter(y); total = sum(cnt.values())\n",
        "    class_weight = {c: total/(len(cnt)*n) for c,n in cnt.items()}\n",
        "    sample_w = [class_weight[int(t)] for t in y]\n",
        "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "def build_loaders(train_ds, val_ds, batch_size=36, num_workers=2):\n",
        "    # На Windows форсим 0 воркеров, иначе повиснет на сериализации gensim-модели\n",
        "    if IS_WIN:\n",
        "        num_workers = 0\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler_from_ds(train_ds),\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=False,          # важно: False\n",
        "        persistent_workers=False,  # важно: False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=False,\n",
        "        persistent_workers=False,\n",
        "    )\n",
        "    print(f\"[Loaders] batch_size={batch_size}, num_workers={num_workers}, pin_memory=False\")\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Тренер v3 (warmup CE → Focal/EMA/SWA + OneCycle, MC-dropout на val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === train_model_v3 с EarlyStopping (по val_macro_f1) ===\n",
        "\n",
        "def train_model_v3(model, train_ds, val_ds, *,\n",
        "                   epochs=24, base_lr=4.8e-4, weight_decay=5e-5, pct_start=0.1,\n",
        "                   use_onecycle=True, use_ema=True, use_swa=False, swa_start_ratio=0.7, swa_lr=None,\n",
        "                   loss_type=\"focal\", focal_gamma=1.6, label_smoothing=0.06, warmup_ce_epochs=5,\n",
        "                   batch_size=32, num_workers=0, ms_dropout_samples=3, class_names=None,\n",
        "                   early_stop=True, es_patience=5, es_min_delta=1e-3):\n",
        "    import math, json\n",
        "    from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "    from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    model = model.to(device)\n",
        "    train_loader, val_loader = build_loaders(train_ds, val_ds, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "    scheduler = (torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=base_lr,\n",
        "                 steps_per_epoch=len(train_loader), epochs=epochs, pct_start=pct_start)\n",
        "                 if use_onecycle else torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs))\n",
        "\n",
        "    ema = EMA(model, decay=0.999) if use_ema else None\n",
        "    swa_model = AveragedModel(model) if use_swa else None\n",
        "    swa_start = int(math.ceil(epochs * swa_start_ratio))\n",
        "\n",
        "    # классовые веса заранее\n",
        "    alpha_vec = make_class_alpha(train_ds, model.fc.out_features)\n",
        "\n",
        "    history = []\n",
        "    best_f1 = -1.0\n",
        "    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    best_ema_shadow = {k: v.detach().cpu().clone() for k, v in ema.shadow.items()} if ema else None\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        if epoch <= warmup_ce_epochs or loss_type == \"ce\":\n",
        "            # CE на прогреве: со сглаживанием + класс-весами\n",
        "            criterion = nn.CrossEntropyLoss(weight=alpha_vec.to(device), label_smoothing=label_smoothing)\n",
        "        else:\n",
        "            # Focal без smoothing (как в оригинале фокала), но с alpha по классам\n",
        "            criterion = FocalLoss(gamma=focal_gamma, alpha=alpha_vec, reduction=\"mean\")\n",
        "\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if ema: ema.update(model)\n",
        "            if use_onecycle: scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # ===== Validation (MC-dropout)\n",
        "        model.eval()\n",
        "        if ema: ema.apply_shadow(model)\n",
        "        def _enable_dropout(m):\n",
        "            if isinstance(m, nn.Dropout):\n",
        "                m.train()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                probs_accum = None\n",
        "                model.apply(_enable_dropout)\n",
        "                for _ in range(ms_dropout_samples):\n",
        "                    logits = model(X)\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    probs_accum = probs if probs_accum is None else (probs_accum + probs)\n",
        "                model.eval()\n",
        "                preds = (probs_accum / ms_dropout_samples).argmax(dim=-1)\n",
        "                y_pred += preds.cpu().tolist()\n",
        "                y_true += y.cpu().tolist()\n",
        "        if ema: ema.restore(model)\n",
        "        if not use_onecycle:\n",
        "            scheduler.step()\n",
        "\n",
        "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        # аккуратно посчитаем loss за эпоху\n",
        "        train_loss = total_loss / max(1, len(train_loader))\n",
        "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_macro_f1\": macro_f1})\n",
        "\n",
        "        # возьмём текущий LR (OneCycle обновляет его по батчам)\n",
        "        curr_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        # определим, какой лосс сейчас используется\n",
        "        is_ce = (epoch <= warmup_ce_epochs) or (loss_type == \"ce\")\n",
        "        print(\n",
        "            f\"[Epoch {epoch}/{epochs}] \"\n",
        "            f\"lr={curr_lr:.2e}  \"\n",
        "            f\"loss_type={'CE' if is_ce else 'Focal'}  \"\n",
        "            f\"train_loss={train_loss:.4f}  \"\n",
        "            f\"val_macro_f1={macro_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        # SWA обновление (если включено)\n",
        "        if use_swa and epoch >= swa_start:\n",
        "            swa_model.update_parameters(model)\n",
        "\n",
        "        # Early Stopping\n",
        "        if macro_f1 > best_f1 + es_min_delta:\n",
        "            best_f1 = macro_f1\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            if ema:\n",
        "                best_ema_shadow = {k: v.detach().cpu().clone() for k, v in ema.shadow.items()}\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if early_stop and no_improve >= es_patience:\n",
        "                print(f\"[EarlyStop] нет улучшений {es_patience} эпох. Лучший val_macro_f1={best_f1:.4f}\")\n",
        "                break\n",
        "\n",
        "    # Откат на лучшие веса\n",
        "    model.load_state_dict(best_state)\n",
        "    if ema and best_ema_shadow is not None:\n",
        "        ema.shadow = best_ema_shadow\n",
        "    if use_swa:\n",
        "        update_bn(train_loader, swa_model, device=device)\n",
        "        model = swa_model\n",
        "\n",
        "    # Финальный отчёт по валидации\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pr = model(X).argmax(-1)\n",
        "            y_pred += pr.cpu().tolist()\n",
        "            y_true += y.cpu().tolist()\n",
        "    rep = classification_report(y_true, y_pred, digits=4, zero_division=0, output_dict=True)\n",
        "    cm = confusion_matrix(y_true, y_pred).tolist()\n",
        "    metrics_blob = {\"history\": history,\n",
        "                    \"classification_report\": rep,\n",
        "                    \"confusion_matrix\": cm,\n",
        "                    \"best_val_macro_f1\": best_f1}\n",
        "    (EXPERIMENTS_DIR/\"last_val_metrics_eng.json\").write_text(json.dumps(metrics_blob, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(\"[trainer] Saved val metrics → experiments/last_val_metrics_eng.json\")\n",
        "    return model, metrics_blob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) TTA и bias tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "925fc901",
      "metadata": {},
      "outputs": [],
      "source": [
        "# пути к исходному ENG CSV и к новому датасету\n",
        "SRC_ENG = Path(r\"eng_toxic_full_df.csv\")       # <-- исходный ENG CSV\n",
        "DST_ENG = Path(r\"english_subset_50.csv\")     # <-- куда сохранить половину"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Subset] read: 159571  -> wrote: 79611  (49.9%)\n",
            "Saved to: english_subset_50.csv\n"
          ]
        }
      ],
      "source": [
        "# нужны только эти колонки (остальные не читаем — экономим RAM)\n",
        "usecols = [\n",
        "    \"comment_text\",\n",
        "    \"neutral\", \"toxic_not_threat\", \"toxic_threat\",\n",
        "]\n",
        "dtypes = {\n",
        "    \"neutral\": \"int8\",\n",
        "    \"toxic_not_threat\": \"int8\",\n",
        "    \"toxic_threat\": \"int8\",\n",
        "}\n",
        "\n",
        "# нормализация (как в RU, плюс @user)\n",
        "URL_RE     = re.compile(r'https?://\\S+')\n",
        "EMOJI_RE   = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
        "TEXT_AT_RE = re.compile(r'@\\w+')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = URL_RE.sub('<URL>', s)\n",
        "    s = EMOJI_RE.sub('<EMOJI>', s)\n",
        "    s = TEXT_AT_RE.sub('<USER>', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "frac = 0.5           # РОВНО половину\n",
        "rng  = np.random.RandomState(42)  # фиксируем сид\n",
        "\n",
        "first = True\n",
        "total_in = total_out = 0\n",
        "for chunk in pd.read_csv(SRC_ENG, usecols=usecols, dtype=dtypes, chunksize=100_000):\n",
        "    total_in += len(chunk)\n",
        "    # берём половину строк из этого чанка\n",
        "    mask = rng.rand(len(chunk)) < frac\n",
        "    sub  = chunk.loc[mask].copy()\n",
        "\n",
        "    if not sub.empty:\n",
        "        sub = sub.rename(columns={\"comment_text\": \"text\"})\n",
        "        sub[\"text\"] = sub[\"text\"].map(normalize_text)\n",
        "        sub.to_csv(DST_ENG, index=False, mode=(\"w\" if first else \"a\"), header=first)\n",
        "        total_out += len(sub)\n",
        "        first = False\n",
        "\n",
        "print(f\"[Subset] read: {total_in}  -> wrote: {total_out}  ({total_out/max(1,total_in):.1%})\")\n",
        "print(\"Saved to:\", DST_ENG)\n",
        "\n",
        "\n",
        "def tta_variants(text: str) -> List[str]:\n",
        "    t = text if isinstance(text, str) else str(text)\n",
        "    cand = [t, t.lower(), URL_RE.sub(\"<URL>\", t), EMOJI_RE.sub(\"<EMOJI>\", t), re.sub(r\"[^\\w\\s]\", \" \", t.lower())]\n",
        "    seen, out = set(), []\n",
        "    for v in cand:\n",
        "        if v not in seen:\n",
        "            out.append(v); seen.add(v)\n",
        "    return out[:7]\n",
        "\n",
        "def predict_proba_tta(model, texts: List[str], ft_model, max_tokens: int, batch=256):\n",
        "    all_probs = []\n",
        "    for txt in texts:\n",
        "        vs = tta_variants(txt)\n",
        "        Xv = [embed_tokens(_tokenize(v), ft_model, max_tokens) for v in vs]\n",
        "        Xv = torch.stack(Xv, dim=0)\n",
        "        dl = DataLoader([(x, 0) for x in Xv], batch_size=batch, shuffle=False)\n",
        "        model.eval()\n",
        "        probs_acc = None\n",
        "        with torch.no_grad():\n",
        "            for X,_ in dl:\n",
        "                X = X.to(device)\n",
        "                logits = model(X)\n",
        "                probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "                probs_acc = probs if probs_acc is None else np.vstack((probs_acc, probs))\n",
        "        all_probs.append(probs_acc.mean(axis=0))\n",
        "    return np.vstack(all_probs)\n",
        "\n",
        "def tune_logit_biases(probs_val: np.ndarray, y_val: List[int], grid=(-0.5, -0.25, 0.0, 0.25, 0.5)):\n",
        "    from itertools import product\n",
        "    C = probs_val.shape[1]\n",
        "    best_f1, best_b = -1.0, np.zeros(C, dtype=np.float32)\n",
        "    for deltas in product(grid, repeat=C):\n",
        "        shifted = probs_val + np.array(deltas)[None, :]\n",
        "        pred = shifted.argmax(1)\n",
        "        f1 = f1_score(y_val, pred, average=\"macro\")\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_b = f1, np.array(deltas, dtype=np.float32)\n",
        "    return best_b, best_f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Чтение прошлых результатов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_previous_runs(exp_dir: Path):\n",
        "    runs = []\n",
        "    for p in [exp_dir/\"runs_eng.jsonl\", Path(\"runs_eng.jsonl\")]:\n",
        "        if p.exists():\n",
        "            with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        runs.append(json.loads(line))\n",
        "                    except:\n",
        "                        pass\n",
        "    return runs\n",
        "\n",
        "def load_best_run(exp_dir: Path):\n",
        "    for p in [exp_dir/\"best_run_eng.json\", Path(\"best_run_eng.json\")]:\n",
        "        if p.exists():\n",
        "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    return None\n",
        "\n",
        "def get_params_for_id(runs: List[Dict[str,Any]], run_id: str) -> Optional[Dict[str,Any]]:\n",
        "    for r in runs:\n",
        "        if isinstance(r, dict):\n",
        "            pp = r.get(\"parameters\")\n",
        "            if isinstance(pp, dict) and pp.get(\"id\") == run_id:\n",
        "                return pp\n",
        "            if r.get(\"id\") == run_id and \"parameters\" in r:\n",
        "                return r[\"parameters\"]\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Оркестратор improve_from_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === improve_from_best: форсим базу = ADV_DEFAULT; опц. свип LR; ансамбль по сид-ам ===\n",
        "from typing import List, Optional\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def improve_from_best(seeds: List[int] = [42,1337,2025],\n",
        "                      save_name: str = \"ensemble_95_target_eng.json\",\n",
        "                      lr_override: Optional[float] = None,\n",
        "                      lr_grid: Optional[List[float]] = None,\n",
        "                      lr_grid_epochs: Optional[int] = None):\n",
        "    # 1) берём пресет как БАЗУ\n",
        "    data_p  = dict(ADV_DEFAULT[\"data\"])\n",
        "    model_p = dict(ADV_DEFAULT[\"model\"])\n",
        "    train_p = dict(ADV_DEFAULT[\"train\"])\n",
        "    class_labels = data_p[\"class_labels\"]\n",
        "\n",
        "    # 2) читаем датасет и сплитим\n",
        "    df = pd.read_csv(data_p[\"dataset_path\"])\n",
        "    texts_all = df[\"text\"].astype(str).tolist()\n",
        "    y_all = np.argmax(df[class_labels].values, axis=1).tolist()\n",
        "\n",
        "    take_fraction = float(data_p.get(\"take_fraction\", 1.0))\n",
        "    if 0.0 < take_fraction < 1.0:\n",
        "        idx = np.arange(len(texts_all))\n",
        "        idx_sub, _ = train_test_split(\n",
        "            idx, train_size=take_fraction, random_state=rs, stratify=y_all\n",
        "        )\n",
        "        texts_all = [texts_all[i] for i in idx_sub]\n",
        "        y_all     = [y_all[i]     for i in idx_sub]\n",
        "        print(f\"[DATA] Using fraction={take_fraction:.2f} -> {len(texts_all)} rows\")\n",
        "\n",
        "    rs = int(data_p.get(\"random_state\", 42))\n",
        "    test_size = float(data_p.get(\"test_size\", 0.2))\n",
        "    val_size  = float(data_p.get(\"val_size\", 0.1))\n",
        "    x_tmp, x_test, y_tmp, y_test = train_test_split(texts_all, y_all, test_size=test_size, random_state=rs, stratify=y_all)\n",
        "\n",
        "    val_rel = val_size / (1.0 - test_size) if (1.0 - test_size) > 0 else 0.0\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "        x_tmp, y_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=rs,\n",
        "        stratify=y_tmp\n",
        "    )\n",
        "    # для единообразия имён:\n",
        "    X_test, Y_test = x_test, y_test\n",
        "    print(f\"[SPLIT] train={len(X_train)}  val={len(X_val)}  test={len(X_test)}\")\n",
        "\n",
        "    rel_val = val_size / (1.0 - test_size)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_tmp, y_tmp, test_size=rel_val, random_state=rs, stratify=y_tmp)\n",
        "\n",
        "    # 3) fastText и стриминговые датасеты\n",
        "    ft = ensure_fasttext_model(str(FASTTEXT_BIN))\n",
        "    max_tokens = int(data_p[\"max_tokens\"])\n",
        "    train_ds = StreamingTensorDataset(x_train, y_train, ft, max_tokens)\n",
        "    val_ds   = StreamingTensorDataset(x_val,   y_val,   ft, max_tokens)\n",
        "    test_ds  = StreamingTensorDataset(x_test,  y_test,  ft, max_tokens)\n",
        "    num_classes = len(class_labels)\n",
        "\n",
        "    # helper — один прогон\n",
        "    def _train_and_probs(seed: int, base_lr: float, epochs_use: int):\n",
        "        set_seed(int(seed))\n",
        "        model = make_model_from_params(model_p, num_classes=num_classes, embed_dim=model_p.get(\"embed_dim\",300))\n",
        "        model, _ = train_model_v3(\n",
        "            model, train_ds, val_ds,\n",
        "            epochs=train_p.get(\"epochs\", 30),           \n",
        "            base_lr=best_lr,\n",
        "            weight_decay=train_p.get(\"weight_decay\", 5e-5),\n",
        "            pct_start=train_p.get(\"pct_start\", 0.05),\n",
        "            use_onecycle=(train_p.get(\"scheduler\",\"onecycle\") == \"onecycle\"),\n",
        "            use_ema=train_p.get(\"use_ema\", True),\n",
        "            use_swa=train_p.get(\"use_swa\", True),       # ← SWA\n",
        "            loss_type=\"focal\",\n",
        "            focal_gamma=train_p.get(\"focal_gamma\", 1.6),\n",
        "            label_smoothing=train_p.get(\"label_smoothing\", 0.06),\n",
        "            warmup_ce_epochs=6,                         # ← подольше прогрев CE\n",
        "            batch_size=min(36, train_p.get(\"batch_size\", 36)),\n",
        "            num_workers=0,\n",
        "            ms_dropout_samples=model_p.get(\"ms_dropout_samples\", 6),\n",
        "            class_names=class_labels,\n",
        "            early_stop=True, es_patience=6, es_min_delta=5e-4\n",
        "        )\n",
        "\n",
        "        pv = predict_proba_tta(model, x_val,  ft, max_tokens)\n",
        "        pt = predict_proba_tta(model, x_test, ft, max_tokens)\n",
        "        return pv, pt\n",
        "\n",
        "    # 4) выбор LR \n",
        "    if lr_override is not None:\n",
        "        best_lr = float(lr_override)\n",
        "        print(f\"[LR] override = {best_lr}\")\n",
        "    elif lr_grid:\n",
        "        probe_seed = int(seeds[0]) if seeds else 42\n",
        "        probe_epochs = int(lr_grid_epochs or min(14, train_p[\"epochs\"]))\n",
        "        print(f\"[LR] grid search on seed={probe_seed}, epochs={probe_epochs}: {lr_grid}\")\n",
        "        best_lr, best_f1 = None, -1.0\n",
        "        for lr in lr_grid:\n",
        "            pv, _ = _train_and_probs(probe_seed, float(lr), probe_epochs)\n",
        "            bias,_ = tune_logit_biases(pv, y_val, grid=(-0.25,-0.1,0.0,0.1,0.25))\n",
        "            f1 = f1_score(y_val, (pv + bias[None,:]).argmax(1), average=\"macro\")\n",
        "            print(f\"  lr={lr:.6f}  val_macro_f1={f1:.4f}\")\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_lr = f1, float(lr)\n",
        "        print(f\"[LR] best = {best_lr:.6f} (val_macro_f1={best_f1:.4f})\")\n",
        "    else:\n",
        "        best_lr = float(train_p[\"learning_rate\"])\n",
        "        print(f\"[LR] from preset = {best_lr}\")\n",
        "\n",
        "    # 5) ансамбль по сид-ам\n",
        "    probs_val_sum = probs_test_sum = None\n",
        "    for s in seeds:\n",
        "        pv, pt = _train_and_probs(int(s), best_lr, epochs_use=min(20, train_p[\"epochs\"]))\n",
        "        probs_val_sum  = pv if probs_val_sum  is None else (probs_val_sum  + pv)\n",
        "        probs_test_sum = pt if probs_test_sum is None else (probs_test_sum + pt)\n",
        "\n",
        "    probs_val_mean  = probs_val_sum  / max(1, len(seeds))\n",
        "    probs_test_mean = probs_test_sum / max(1, len(seeds))\n",
        "\n",
        "    bias, _ = tune_logit_biases(probs_val_mean, y_val,\n",
        "                                grid=(-0.25, -0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.25))\n",
        "    val_pred  = (probs_val_mean  + bias[None,:]).argmax(1)\n",
        "    test_pred = (probs_test_mean + bias[None,:]).argmax(1)\n",
        "\n",
        "    val_f1   = f1_score(y_val,  val_pred,  average=\"macro\")\n",
        "    test_f1  = f1_score(y_test, test_pred, average=\"macro\")\n",
        "    val_acc  = accuracy_score(y_val,  val_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    out = {\n",
        "        \"base_id\": \"auto_advanced_medium\",\n",
        "        \"seeds\": list(map(int, seeds)),\n",
        "        \"val_macro_f1\": float(val_f1),\n",
        "        \"val_accuracy\": float(val_acc),\n",
        "        \"test_macro_f1\": float(test_f1),\n",
        "        \"test_accuracy\": float(test_acc),\n",
        "        \"bias\": bias.tolist(),\n",
        "        \"used_params\": {\"data\":data_p, \"model\":model_p, \"train\":train_p},\n",
        "        \"chosen_lr\": best_lr,\n",
        "        \"notes\": f\"Advanced preset + Ensemble x{len(seeds)} + TTA + bias (num_workers=0)\"\n",
        "    }\n",
        "    (EXPERIMENTS_DIR/save_name).write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"[RESULT] VAL F1={val_f1:.4f}  ACC={val_acc:.4f} | TEST F1={test_f1:.4f}  ACC={test_acc:.4f} | LR={best_lr}\")\n",
        "    print(f\"[SAVED] {EXPERIMENTS_DIR/save_name}\")\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Запуск улучшения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SPLIT] train=55727  val=7961  test=15923\n",
            "[FT] Using native fasttext (subword).\n",
            "[LR] override = 0.0008\n",
            "[Seed] set to 42\n",
            "[Loaders] batch_size=36, num_workers=0, pin_memory=False\n",
            "[Epoch 1/30] lr=6.08e-04  loss_type=CE  train_loss=0.1515  val_macro_f1=0.0046\n",
            "[Epoch 2/30] lr=7.99e-04  loss_type=CE  train_loss=0.0761  val_macro_f1=0.0852\n",
            "[Epoch 3/30] lr=7.95e-04  loss_type=CE  train_loss=0.0657  val_macro_f1=0.2141\n",
            "[Epoch 4/30] lr=7.85e-04  loss_type=CE  train_loss=0.0621  val_macro_f1=0.2427\n",
            "[Epoch 5/30] lr=7.71e-04  loss_type=CE  train_loss=0.0604  val_macro_f1=0.2469\n",
            "[Epoch 6/30] lr=7.52e-04  loss_type=CE  train_loss=0.0605  val_macro_f1=0.2454\n",
            "[Epoch 7/30] lr=7.29e-04  loss_type=Focal  train_loss=0.0023  val_macro_f1=0.5961\n",
            "[Epoch 8/30] lr=7.02e-04  loss_type=Focal  train_loss=0.0016  val_macro_f1=0.6150\n",
            "[Epoch 9/30] lr=6.71e-04  loss_type=Focal  train_loss=0.0029  val_macro_f1=0.5745\n",
            "[Epoch 10/30] lr=6.37e-04  loss_type=Focal  train_loss=0.0027  val_macro_f1=0.6031\n",
            "[Epoch 11/30] lr=6.00e-04  loss_type=Focal  train_loss=0.0011  val_macro_f1=0.5967\n",
            "[Epoch 12/30] lr=5.61e-04  loss_type=Focal  train_loss=0.0013  val_macro_f1=0.6139\n",
            "[Epoch 13/30] lr=5.19e-04  loss_type=Focal  train_loss=0.0014  val_macro_f1=0.6284\n",
            "[Epoch 14/30] lr=4.77e-04  loss_type=Focal  train_loss=0.0008  val_macro_f1=0.6309\n",
            "[Epoch 15/30] lr=4.33e-04  loss_type=Focal  train_loss=0.0012  val_macro_f1=0.6304\n",
            "[Epoch 16/30] lr=3.89e-04  loss_type=Focal  train_loss=0.0011  val_macro_f1=0.6417\n",
            "[Epoch 17/30] lr=3.45e-04  loss_type=Focal  train_loss=0.0006  val_macro_f1=0.6470\n",
            "[Epoch 18/30] lr=3.02e-04  loss_type=Focal  train_loss=0.0006  val_macro_f1=0.6592\n",
            "[Epoch 19/30] lr=2.60e-04  loss_type=Focal  train_loss=0.0005  val_macro_f1=0.6637\n",
            "[Epoch 20/30] lr=2.19e-04  loss_type=Focal  train_loss=0.0004  val_macro_f1=0.6735\n",
            "[Epoch 21/30] lr=1.81e-04  loss_type=Focal  train_loss=0.0004  val_macro_f1=0.6857\n",
            "[Epoch 22/30] lr=1.46e-04  loss_type=Focal  train_loss=0.0004  val_macro_f1=0.6932\n",
            "[Epoch 23/30] lr=1.13e-04  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.6772\n",
            "[Epoch 24/30] lr=8.43e-05  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.6952\n",
            "[Epoch 25/30] lr=5.92e-05  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.7017\n",
            "[Epoch 26/30] lr=3.82e-05  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.6911\n",
            "[Epoch 27/30] lr=2.17e-05  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.7039\n",
            "[Epoch 28/30] lr=9.68e-06  loss_type=Focal  train_loss=0.0002  val_macro_f1=0.6932\n",
            "[Epoch 29/30] lr=2.43e-06  loss_type=Focal  train_loss=0.0002  val_macro_f1=0.6942\n",
            "[Epoch 30/30] lr=3.20e-09  loss_type=Focal  train_loss=0.0003  val_macro_f1=0.7073\n",
            "[trainer] Saved val metrics → experiments/last_val_metrics_eng.json\n",
            "[RESULT] VAL F1=0.7184  ACC=0.9449 | TEST F1=0.7225  ACC=0.9471 | LR=0.0008\n",
            "[SAVED] e:\\Python_projects\\Toxic_class_base\\experiments_en\\ensemble_95_target_eng.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'base_id': 'auto_advanced_medium',\n",
              " 'seeds': [42],\n",
              " 'val_macro_f1': 0.7184151536881306,\n",
              " 'val_accuracy': 0.9448561738475066,\n",
              " 'test_macro_f1': 0.7225389873915398,\n",
              " 'test_accuracy': 0.9471205174904227,\n",
              " 'bias': [0.25, -0.25, 0.25],\n",
              " 'used_params': {'data': {'sample_per_class': 16000,\n",
              "   'max_tokens': 160,\n",
              "   'test_size': 0.2,\n",
              "   'val_size': 0.1,\n",
              "   'random_state': 42,\n",
              "   'class_labels': ['neutral', 'toxic_not_threat', 'toxic_threat'],\n",
              "   'dataset_path': 'e:\\\\Python_projects\\\\Toxic_class_base\\\\english_subset_50.csv',\n",
              "   'take_fraction': 1.0},\n",
              "  'model': {'embed_dim': 300,\n",
              "   'hidden_size': 320,\n",
              "   'num_layers': 2,\n",
              "   'dropout': 0.28,\n",
              "   'use_advanced_model': True,\n",
              "   'use_advanced_attention': True,\n",
              "   'ms_dropout_samples': 6},\n",
              "  'train': {'batch_size': 36,\n",
              "   'epochs': 30,\n",
              "   'learning_rate': 0.0008,\n",
              "   'weight_decay': 5e-05,\n",
              "   'scheduler': 'onecycle',\n",
              "   'pct_start': 0.05,\n",
              "   'use_ema': True,\n",
              "   'use_swa': True,\n",
              "   'focal_gamma': 1.6,\n",
              "   'label_smoothing': 0.06}},\n",
              " 'chosen_lr': 0.0008,\n",
              " 'notes': 'Advanced preset + Ensemble x1 + TTA + bias (num_workers=0)'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = improve_from_best(seeds=[42], lr_override=8e-4)\n",
        "res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9bf67d9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Итог для отчёта ===\n",
            "Test accuracy (главный критерий задания): 0.9471\n",
            "Test macro-F1: 0.7225\n",
            "Val macro-F1 (для подбора модели): 0.7184\n",
            "Val accuracy: 0.9449\n",
            "\n",
            "[Saved] report/summary_metrics_eng.json\n"
          ]
        }
      ],
      "source": [
        "# === Итоговые метрики для отчёта ===\n",
        "import json, os, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "EXP = Path(\"experiments\")\n",
        "res = globals().get(\"res\", None) \n",
        "\n",
        "\n",
        "val_f1 = test_f1 = val_acc = test_acc = None\n",
        "if isinstance(res, dict):\n",
        "    val_f1  = float(res.get(\"val_macro_f1\", np.nan))\n",
        "    test_f1 = float(res.get(\"test_macro_f1\", np.nan))\n",
        "    val_acc = float(res.get(\"val_accuracy\", np.nan))\n",
        "    test_acc= float(res.get(\"test_accuracy\", np.nan))\n",
        "\n",
        "\n",
        "if (val_f1 is None or np.isnan(val_f1)) and (EXP/\"last_val_metrics_eng.json\").exists():\n",
        "    j = json.loads((EXP/\"last_val_metrics_eng.json\").read_text(encoding=\"utf-8\"))\n",
        "    val_f1 = float(j.get(\"best_val_macro_f1\", np.nan))\n",
        "if (test_f1 is None or np.isnan(test_f1)) and (EXP/\"ensemble_95_target_eng.json\").exists():\n",
        "    j = json.loads((EXP/\"ensemble_95_target_eng.json\").read_text(encoding=\"utf-8\"))\n",
        "    test_f1  = float(j.get(\"test_macro_f1\", np.nan))\n",
        "    test_acc = float(j.get(\"test_accuracy\", np.nan))\n",
        "    val_acc  = float(j.get(\"val_accuracy\", np.nan)) if val_acc is None else val_acc\n",
        "\n",
        "# Печать короткого сводного блока\n",
        "print(\"=== Итог для отчёта ===\")\n",
        "print(f\"Test accuracy (главный критерий задания): {test_acc:.4f}\" if test_acc is not None else \"Test accuracy: n/a\")\n",
        "print(f\"Test macro-F1: {test_f1:.4f}\" if test_f1 is not None else \"Test macro-F1: n/a\")\n",
        "print(f\"Val macro-F1 (для подбора модели): {val_f1:.4f}\" if val_f1 is not None else \"Val macro-F1: n/a\")\n",
        "print(f\"Val accuracy: {val_acc:.4f}\" if val_acc is not None else \"Val accuracy: n/a\")\n",
        "\n",
        "# Если есть подробный classification_report/CM из последнего валидационного прогона\n",
        "report_path = EXP/\"last_val_metrics_eng.json\"\n",
        "if report_path.exists():\n",
        "    j = json.loads(report_path.read_text(encoding=\"utf-8\"))\n",
        "    cr = j.get(\"classification_report\")\n",
        "    cm = j.get(\"confusion_matrix\")\n",
        "    if cr:\n",
        "        print(\"\\n— Per-class на валидации (macro-F1 мы использовали для выбора):\")\n",
        "        df_cr = pd.DataFrame(cr).T\n",
        "        display(df_cr)\n",
        "    if cm:\n",
        "        print(\"— Confusion matrix (валидация):\")\n",
        "        df_cm = pd.DataFrame(cm,\n",
        "                             index=[\"true:neutral\",\"true:not_threat\",\"true:threat\"],\n",
        "                             columns=[\"pred:neutral\",\"pred:not_threat\",\"pred:threat\"])\n",
        "        display(df_cm)\n",
        "\n",
        "# Сохраним мини-выжимку для вставки в отчёт\n",
        "REPORT = Path(\"report\"); REPORT.mkdir(exist_ok=True)\n",
        "summary = {\n",
        "    \"test_accuracy\": test_acc,\n",
        "    \"test_macro_f1\": test_f1,\n",
        "    \"val_macro_f1\": val_f1,\n",
        "    \"val_accuracy\": val_acc,\n",
        "}\n",
        "(Path(REPORT/\"summary_metrics_eng.json\")).write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(f\"\\n[Saved] report/summary_metrics_eng.json\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (toxic-base)",
      "language": "python",
      "name": "toxic-base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
